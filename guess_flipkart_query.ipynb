{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---general purpose libraries--\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#----for model bulding -----------\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#---for model evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#--for text processing----\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "#---for pipelining-----\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Internshala/Mesh\n"
     ]
    }
   ],
   "source": [
    "#--change the directory to your working directory where you have your training, sample input files\n",
    "working_dir = input()\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------FUNCTION DEFINITIONS -------------------\n",
    "\n",
    "#--function to preprocess text\n",
    "#--> Removes numbers from the text\n",
    "#--> Removes punctuation marks from the text\n",
    "#--> Returns array\n",
    "def preprocess(text):\n",
    "    return re.findall(r'(?:[a-zA-Z]+[a-zA-Z\\'\\-]?[a-zA-Z]|[a-zA-Z]+)',text)\n",
    "\n",
    "#--function to print performnace report-\n",
    "def performance_report(true,pred):\n",
    "    print(confusion_matrix(true, pred))\n",
    "    print(classification_report(true, pred))\n",
    "#----------FUNCTION DEFINITIONS ENDS HERE-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the file which has user query and retreived procuct to create 2 create 2 different arrays product and user_query\n",
    "training = open('training.txt')\n",
    "product=[]\n",
    "user_query=[]\n",
    "first_line = 1\n",
    "for line in training:\n",
    "    if first_line==1:\n",
    "        first_line = 0\n",
    "        continue\n",
    "    input_txt = line.strip().split(\"\\t\")\n",
    "    processed_txt = ' '.join(txt for txt in preprocess(input_txt[0]))\n",
    "    product.append(processed_txt)\n",
    "    user_query.append(input_txt[1])\n",
    "x_train=product\n",
    "y_train=user_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tru...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pipeline for vectorizing the text(also removes stop words) input and then creating model sequentially\n",
    "x_train=product\n",
    "y_train=user_query\n",
    "\n",
    "#pred_model=Pipeline([('vectorize',TfidfVectorizer(stop_words='english')),('predictive_model', MultinomialNB())])\n",
    "pred_model=Pipeline([('vectorize',TfidfVectorizer(stop_words='english')),\n",
    "                   ('predictive_model', RandomForestClassifier())])\n",
    "pred_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--process the sample input--\n",
    "#--apply preprocessing on the text\n",
    "test_prdct=[]\n",
    "sample_input = open('Sample_Input.txt')\n",
    "\n",
    "first_line = 1\n",
    "for line in sample_input:\n",
    "    if first_line==1:\n",
    "        first_line = 0\n",
    "        continue\n",
    "    input_txt = line.strip()\n",
    "    txt = ' '.join(preprocess(input_txt))\n",
    "    test_prdct.append(txt)\n",
    "x_test = np.array(test_prdct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--predict the corresponding query with the model\n",
    "pred_query=pred_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import sample output to compare with predicted output\n",
    "test_user_query = []\n",
    "sample_output = open('Sample_Output.txt') \n",
    "for line in sample_output:\n",
    "    test_user_query.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--create dataframe to store the predicted Vs actual result\n",
    "df_predicted_vs_actual = pd.DataFrame(\n",
    "    {'Product': test_prdct,\n",
    "     'Actual_User_Query': test_user_query,\n",
    "     'Predicted_User_Query': pred_query\n",
    "    })\n",
    "#df_predicted_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 6 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                   axe deo       0.75      0.60      0.67         5\n",
      "         best-seller books       0.40      1.00      0.57         2\n",
      "             c programming       0.60      0.50      0.55         6\n",
      "              calvin klein       1.00      1.00      1.00         5\n",
      "                 camcorder       1.00      1.00      1.00         2\n",
      "                    camera       0.33      1.00      0.50         1\n",
      "                 chemistry       1.00      0.86      0.92         7\n",
      "                chromebook       0.00      0.00      0.00         1\n",
      "data structures algorithms       1.00      1.00      1.00         1\n",
      "              dell laptops       0.60      1.00      0.75         3\n",
      "                dslr canon       1.00      1.00      1.00         2\n",
      "               mathematics       0.60      0.60      0.60         5\n",
      "             nike-deodrant       0.00      0.00      0.00         1\n",
      "                   physics       0.75      0.60      0.67         5\n",
      "            sony cybershot       1.00      1.00      1.00         5\n",
      "            spoken english       1.00      1.00      1.00         3\n",
      "               timex watch       1.00      1.00      1.00         1\n",
      "               titan watch       1.00      1.00      1.00         3\n",
      "           written english       1.00      0.50      0.67         2\n",
      "\n",
      "               avg / total       0.80      0.78      0.78        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#--print the performance report\n",
    "performance_report(df_predicted_vs_actual['Actual_User_Query'],df_predicted_vs_actual['Predicted_User_Query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
